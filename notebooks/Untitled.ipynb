{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev3 toc-item\"><a href=\"#Based-on-the-following-notebooks\" data-toc-modified-id=\"Based-on-the-following-notebooks-001\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>Based on the following notebooks</a></div><div class=\"lev3 toc-item\"><a href=\"#Load-input-data\" data-toc-modified-id=\"Load-input-data-002\"><span class=\"toc-item-num\">0.0.2&nbsp;&nbsp;</span>Load input data</a></div><div class=\"lev3 toc-item\"><a href=\"#Merge-DFs\" data-toc-modified-id=\"Merge-DFs-003\"><span class=\"toc-item-num\">0.0.3&nbsp;&nbsp;</span>Merge DFs</a></div><div class=\"lev3 toc-item\"><a href=\"#Add-features\" data-toc-modified-id=\"Add-features-004\"><span class=\"toc-item-num\">0.0.4&nbsp;&nbsp;</span>Add features</a></div><div class=\"lev3 toc-item\"><a href=\"#Split-train-and-validation-set\" data-toc-modified-id=\"Split-train-and-validation-set-005\"><span class=\"toc-item-num\">0.0.5&nbsp;&nbsp;</span>Split train and validation set</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on the following notebooks\n",
    "\n",
    "https://www.kaggle.com/asmitavikas/feature-engineered-0-68310\n",
    "\n",
    "https://www.kaggle.com/vinnsvinay/introduction-to-boosting-using-lgbm-lb-0-68357\n",
    "\n",
    "https://www.kaggle.com/kamilkk/i-have-to-say-this\n",
    "\n",
    "Python 2.7\n",
    "Pandas 0.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import datetime\n",
    "import math\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "members.csv\n",
      "sample_submission.csv\n",
      "song_extra_info.csv\n",
      "songs.csv\n",
      "songs.fixed.csv\n",
      "submission_lgbm_avg.csv.gz\n",
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../input/'\n",
    "# really? msno, song_id etc. as category?\n",
    "train = pd.read_csv(data_path + 'train.csv', dtype={'msno' : 'category',\n",
    "                                                'source_system_tab' : 'category',\n",
    "                                                  'source_screen_name' : 'category',\n",
    "                                                  'source_type' : 'category',\n",
    "                                                  'target' : np.uint8,\n",
    "                                                  'song_id' : 'category'})\n",
    "test = pd.read_csv(data_path + 'test.csv', dtype={'msno' : 'category',\n",
    "                                                'source_system_tab' : 'category',\n",
    "                                                'source_screen_name' : 'category',\n",
    "                                                'source_type' : 'category',\n",
    "                                                'song_id' : 'category'})\n",
    "# Fix songs list https://www.kaggle.com/alexklibisz/songs-csv-quote-errors-cause-513-missing-songs\n",
    "songs = pd.read_csv(data_path + 'songs.fixed.csv',dtype={'genre_ids': 'category',\n",
    "                                                  'language' : 'category',\n",
    "                                                  'artist_name' : 'category',\n",
    "                                                  'composer' : 'category',\n",
    "                                                  'lyricist' : 'category',\n",
    "                                                  'song_id' : 'category'})\n",
    "members = pd.read_csv(data_path + 'members.csv',dtype={'city' : 'category',\n",
    "                                                      'bd' : np.uint8,\n",
    "                                                      'gender' : 'category',\n",
    "                                                      'registered_via' : 'category'},\n",
    "                     parse_dates=['registration_init_time','expiration_date'])\n",
    "songs_extra = pd.read_csv(data_path + 'song_extra_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(songs, on='song_id', how='left')\n",
    "test = test.merge(songs, on='song_id', how='left')\n",
    "\n",
    "members['membership_days'] = members['expiration_date'].subtract(members['registration_init_time']).dt.days.astype(int)\n",
    "\n",
    "members['registration_year'] = members['registration_init_time'].dt.year\n",
    "members['registration_month'] = members['registration_init_time'].dt.month\n",
    "members['registration_date'] = members['registration_init_time'].dt.day\n",
    "\n",
    "members['expiration_year'] = members['expiration_date'].dt.year\n",
    "members['expiration_month'] = members['expiration_date'].dt.month\n",
    "members['expiration_date'] = members['expiration_date'].dt.day\n",
    "members = members.drop(['registration_init_time'], axis=1)\n",
    "\n",
    "def isrc_to_year(isrc):\n",
    "    if type(isrc) == str:\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan\n",
    "        \n",
    "songs_extra['song_year'] = songs_extra['isrc'].apply(isrc_to_year)\n",
    "songs_extra.drop(['isrc', 'name'], axis = 1, inplace = True)\n",
    "\n",
    "train = train.merge(members, on='msno', how='left')\n",
    "test = test.merge(members, on='msno', how='left')\n",
    "\n",
    "train = train.merge(songs_extra, on = 'song_id', how = 'left')\n",
    "train.song_length.fillna(200000,inplace=True)\n",
    "train.song_length = train.song_length.astype(np.uint32)\n",
    "train.song_id = train.song_id.astype('category')\n",
    "\n",
    "\n",
    "test = test.merge(songs_extra, on = 'song_id', how = 'left')\n",
    "test.song_length.fillna(200000,inplace=True)\n",
    "test.song_length = test.song_length.astype(np.uint32)\n",
    "test.song_id = test.song_id.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_id_count(x):\n",
    "    if x == 'no_genre_id':\n",
    "        return 0\n",
    "    else:\n",
    "        return x.count('|') + 1\n",
    "\n",
    "train['genre_ids'].fillna('no_genre_id',inplace=True)\n",
    "test['genre_ids'].fillna('no_genre_id',inplace=True)\n",
    "train['genre_ids_count'] = train['genre_ids'].apply(genre_id_count).astype(np.int8)\n",
    "test['genre_ids_count'] = test['genre_ids'].apply(genre_id_count).astype(np.int8)\n",
    "\n",
    "def lyricist_count(x):\n",
    "    if x == 'no_lyricist':\n",
    "        return 0\n",
    "    else:\n",
    "        return sum(map(x.count, ['|', '/', '\\\\', ';'])) + 1\n",
    "    return sum(map(x.count, ['|', '/', '\\\\', ';']))\n",
    "\n",
    "train['lyricist'].fillna('no_lyricist',inplace=True)\n",
    "test['lyricist'].fillna('no_lyricist',inplace=True)\n",
    "train['lyricists_count'] = train['lyricist'].apply(lyricist_count).astype(np.int8)\n",
    "test['lyricists_count'] = test['lyricist'].apply(lyricist_count).astype(np.int8)\n",
    "\n",
    "def composer_count(x):\n",
    "    if x == 'no_composer':\n",
    "        return 0\n",
    "    else:\n",
    "        return sum(map(x.count, ['|', '/', '\\\\', ';'])) + 1\n",
    "\n",
    "train['composer'].fillna('no_composer',inplace=True)\n",
    "test['composer'].fillna('no_composer',inplace=True)\n",
    "train['composer_count'] = train['composer'].apply(composer_count).astype(np.int8)\n",
    "test['composer_count'] = test['composer'].apply(composer_count).astype(np.int8)\n",
    "\n",
    "def is_featured(x):\n",
    "    if 'feat' in str(x) :\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "train['artist_name'].fillna('no_artist',inplace=True)\n",
    "test['artist_name'].fillna('no_artist',inplace=True)\n",
    "train['is_featured'] = train['artist_name'].apply(is_featured).astype(np.int8)\n",
    "test['is_featured'] = test['artist_name'].apply(is_featured).astype(np.int8)\n",
    "\n",
    "def artist_count(x):\n",
    "    if x == '佚名':\n",
    "        return 0\n",
    "    if x == '群星':\n",
    "        return -1\n",
    "    return sum(map(x.count, ['|', ',', 'feat', '&', 'and'])) + 1\n",
    "\n",
    "train['artist_count'] = train['artist_name'].apply(artist_count).astype(np.int8)\n",
    "test['artist_count'] = test['artist_name'].apply(artist_count).astype(np.int8)\n",
    "\n",
    "# if artist is same as composer\n",
    "train['artist_composer'] = (train['artist_name'] == train['composer']).astype(np.int8)\n",
    "test['artist_composer'] = (test['artist_name'] == test['composer']).astype(np.int8)\n",
    "\n",
    "\n",
    "# if artist, lyricist and composer are all three same\n",
    "train['artist_composer_lyricist'] = ((train['artist_name'] == train['composer']) & (train['artist_name'] == train['lyricist']) & (train['composer'] == train['lyricist'])).astype(np.int8)\n",
    "test['artist_composer_lyricist'] = ((test['artist_name'] == test['composer']) & (test['artist_name'] == test['lyricist']) & (test['composer'] == test['lyricist'])).astype(np.int8)\n",
    "\n",
    "# is song language 17 or 45. \n",
    "def song_lang_boolean(x):\n",
    "    if '17.0' in str(x) or '45.0' in str(x):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "train['song_lang_boolean'] = train['language'].apply(song_lang_boolean).astype(np.int8)\n",
    "test['song_lang_boolean'] = test['language'].apply(song_lang_boolean).astype(np.int8)\n",
    "\n",
    "\n",
    "_mean_song_length = np.mean(train['song_length'])\n",
    "def smaller_song(x):\n",
    "    if x < _mean_song_length:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "train['smaller_song'] = train['song_length'].apply(smaller_song).astype(np.int8)\n",
    "test['smaller_song'] = test['song_length'].apply(smaller_song).astype(np.int8)\n",
    "\n",
    "# number of times a song has been played before\n",
    "_dict_count_song_played_train = {k: v for k, v in train['song_id'].value_counts().iteritems()}\n",
    "_dict_count_song_played_test = {k: v for k, v in test['song_id'].value_counts().iteritems()}\n",
    "def count_song_played(x):\n",
    "    try:\n",
    "        return _dict_count_song_played_train[x]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            return _dict_count_song_played_test[x]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "    \n",
    "\n",
    "train['count_song_played'] = train['song_id'].apply(count_song_played).astype(np.int64)\n",
    "test['count_song_played'] = test['song_id'].apply(count_song_played).astype(np.int64)\n",
    "\n",
    "# number of times the artist has been played\n",
    "_dict_count_artist_played_train = {k: v for k, v in train['artist_name'].value_counts().iteritems()}\n",
    "_dict_count_artist_played_test = {k: v for k, v in test['artist_name'].value_counts().iteritems()}\n",
    "def count_artist_played(x):\n",
    "    try:\n",
    "        return _dict_count_artist_played_train[x]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            return _dict_count_artist_played_test[x]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "train['count_artist_played'] = train['artist_name'].apply(count_artist_played).astype(np.int64)\n",
    "test['count_artist_played'] = test['artist_name'].apply(count_artist_played).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tracks the artist has\n",
    "_artist_tracks_dict = {k: v for k, v in songs['artist_name'].value_counts().iteritems()}\n",
    "def count_artist_tracks(artist_name):\n",
    "    try:\n",
    "        return _artist_tracks_dict[artist_name]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "train['count_artist_tracks'] = train['artist_name'].apply(count_artist_tracks).astype(np.int64)\n",
    "test['count_artist_tracks'] = test['artist_name'].apply(count_artist_tracks).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: one-hot encode genre ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    if train[col].dtype == object:\n",
    "        train[col] = train[col].astype('category')\n",
    "        test[col] = test[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoxli/anaconda3/envs/wsdm-music-recommendation-py27/lib/python2.7/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/xiaoxli/anaconda3/envs/wsdm-music-recommendation-py27/lib/python2.7/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/Users/xiaoxli/anaconda3/envs/wsdm-music-recommendation-py27/lib/python2.7/site-packages/lightgbm/basic.py:671: UserWarning: categorical_feature in param dict is overrided.\n",
      "  warnings.warn('categorical_feature in param dict is overrided.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's auc: 0.758356\n",
      "[20]\tvalid_0's auc: 0.778414\n",
      "[30]\tvalid_0's auc: 0.789859\n",
      "[40]\tvalid_0's auc: 0.796579\n",
      "[50]\tvalid_0's auc: 0.799939\n",
      "[60]\tvalid_0's auc: 0.802865\n",
      "[70]\tvalid_0's auc: 0.804663\n",
      "[80]\tvalid_0's auc: 0.806404\n",
      "[90]\tvalid_0's auc: 0.807767\n",
      "[100]\tvalid_0's auc: 0.80932\n",
      "[110]\tvalid_0's auc: 0.810448\n",
      "[120]\tvalid_0's auc: 0.811214\n",
      "[130]\tvalid_0's auc: 0.812269\n",
      "[140]\tvalid_0's auc: 0.813348\n",
      "[150]\tvalid_0's auc: 0.814004\n",
      "[160]\tvalid_0's auc: 0.81485\n",
      "[170]\tvalid_0's auc: 0.81534\n",
      "[180]\tvalid_0's auc: 0.815586\n",
      "[190]\tvalid_0's auc: 0.816017\n",
      "[200]\tvalid_0's auc: 0.816489\n",
      "[210]\tvalid_0's auc: 0.81689\n",
      "[220]\tvalid_0's auc: 0.817262\n",
      "[230]\tvalid_0's auc: 0.817582\n",
      "[240]\tvalid_0's auc: 0.818002\n",
      "[250]\tvalid_0's auc: 0.818303\n",
      "[260]\tvalid_0's auc: 0.818578\n",
      "[270]\tvalid_0's auc: 0.818872\n",
      "[280]\tvalid_0's auc: 0.819127\n",
      "[290]\tvalid_0's auc: 0.819452\n",
      "[300]\tvalid_0's auc: 0.819674\n",
      "[10]\tvalid_0's auc: 0.759986\n",
      "[20]\tvalid_0's auc: 0.779031\n",
      "[30]\tvalid_0's auc: 0.79014\n",
      "[40]\tvalid_0's auc: 0.796592\n",
      "[50]\tvalid_0's auc: 0.800534\n",
      "[60]\tvalid_0's auc: 0.803335\n",
      "[70]\tvalid_0's auc: 0.805511\n",
      "[80]\tvalid_0's auc: 0.807174\n",
      "[90]\tvalid_0's auc: 0.808737\n",
      "[100]\tvalid_0's auc: 0.81014\n",
      "[110]\tvalid_0's auc: 0.811528\n",
      "[120]\tvalid_0's auc: 0.812464\n",
      "[130]\tvalid_0's auc: 0.813375\n",
      "[140]\tvalid_0's auc: 0.814127\n",
      "[150]\tvalid_0's auc: 0.814733\n",
      "[160]\tvalid_0's auc: 0.815214\n",
      "[170]\tvalid_0's auc: 0.8157\n",
      "[180]\tvalid_0's auc: 0.816065\n",
      "[190]\tvalid_0's auc: 0.816523\n",
      "[200]\tvalid_0's auc: 0.81687\n",
      "[210]\tvalid_0's auc: 0.817118\n",
      "[220]\tvalid_0's auc: 0.817522\n",
      "[230]\tvalid_0's auc: 0.817856\n",
      "[240]\tvalid_0's auc: 0.818128\n",
      "[250]\tvalid_0's auc: 0.818339\n",
      "[260]\tvalid_0's auc: 0.818717\n",
      "[270]\tvalid_0's auc: 0.819074\n",
      "[280]\tvalid_0's auc: 0.819284\n",
      "[290]\tvalid_0's auc: 0.819586\n",
      "[300]\tvalid_0's auc: 0.819798\n",
      "[10]\tvalid_0's auc: 0.759205\n",
      "[20]\tvalid_0's auc: 0.778868\n",
      "[30]\tvalid_0's auc: 0.790711\n",
      "[40]\tvalid_0's auc: 0.797265\n",
      "[50]\tvalid_0's auc: 0.800692\n",
      "[60]\tvalid_0's auc: 0.802912\n",
      "[70]\tvalid_0's auc: 0.804642\n",
      "[80]\tvalid_0's auc: 0.806095\n",
      "[90]\tvalid_0's auc: 0.807806\n",
      "[100]\tvalid_0's auc: 0.809466\n",
      "[110]\tvalid_0's auc: 0.810934\n",
      "[120]\tvalid_0's auc: 0.811771\n",
      "[130]\tvalid_0's auc: 0.812543\n",
      "[140]\tvalid_0's auc: 0.813423\n",
      "[150]\tvalid_0's auc: 0.814224\n",
      "[160]\tvalid_0's auc: 0.814727\n",
      "[170]\tvalid_0's auc: 0.815142\n",
      "[180]\tvalid_0's auc: 0.815505\n",
      "[190]\tvalid_0's auc: 0.816044\n",
      "[200]\tvalid_0's auc: 0.816441\n",
      "[210]\tvalid_0's auc: 0.816765\n",
      "[220]\tvalid_0's auc: 0.817037\n",
      "[230]\tvalid_0's auc: 0.817363\n",
      "[240]\tvalid_0's auc: 0.817614\n",
      "[250]\tvalid_0's auc: 0.817992\n",
      "[260]\tvalid_0's auc: 0.818351\n",
      "[270]\tvalid_0's auc: 0.818621\n",
      "[280]\tvalid_0's auc: 0.818839\n",
      "[290]\tvalid_0's auc: 0.819108\n",
      "[300]\tvalid_0's auc: 0.819246\n",
      "[10]\tvalid_0's auc: 0.758937\n",
      "[20]\tvalid_0's auc: 0.779227\n",
      "[30]\tvalid_0's auc: 0.791005\n",
      "[40]\tvalid_0's auc: 0.796913\n",
      "[50]\tvalid_0's auc: 0.800426\n",
      "[60]\tvalid_0's auc: 0.802466\n",
      "[70]\tvalid_0's auc: 0.805088\n",
      "[80]\tvalid_0's auc: 0.806915\n",
      "[90]\tvalid_0's auc: 0.808246\n",
      "[100]\tvalid_0's auc: 0.809665\n",
      "[110]\tvalid_0's auc: 0.810984\n",
      "[120]\tvalid_0's auc: 0.811575\n",
      "[130]\tvalid_0's auc: 0.812761\n",
      "[140]\tvalid_0's auc: 0.813665\n",
      "[150]\tvalid_0's auc: 0.814475\n",
      "[160]\tvalid_0's auc: 0.815197\n",
      "[170]\tvalid_0's auc: 0.815663\n",
      "[180]\tvalid_0's auc: 0.816018\n",
      "[190]\tvalid_0's auc: 0.816351\n",
      "[200]\tvalid_0's auc: 0.816787\n",
      "[210]\tvalid_0's auc: 0.81703\n",
      "[220]\tvalid_0's auc: 0.817378\n",
      "[230]\tvalid_0's auc: 0.817869\n",
      "[240]\tvalid_0's auc: 0.81822\n",
      "[250]\tvalid_0's auc: 0.818494\n",
      "[260]\tvalid_0's auc: 0.818764\n",
      "[270]\tvalid_0's auc: 0.819038\n",
      "[280]\tvalid_0's auc: 0.819355\n",
      "[290]\tvalid_0's auc: 0.819796\n",
      "[300]\tvalid_0's auc: 0.820013\n",
      "[10]\tvalid_0's auc: 0.759684\n",
      "[20]\tvalid_0's auc: 0.779273\n",
      "[30]\tvalid_0's auc: 0.791516\n",
      "[40]\tvalid_0's auc: 0.797637\n",
      "[50]\tvalid_0's auc: 0.80133\n",
      "[60]\tvalid_0's auc: 0.804035\n",
      "[70]\tvalid_0's auc: 0.805954\n",
      "[80]\tvalid_0's auc: 0.807926\n",
      "[90]\tvalid_0's auc: 0.809222\n",
      "[100]\tvalid_0's auc: 0.810574\n",
      "[110]\tvalid_0's auc: 0.811795\n",
      "[120]\tvalid_0's auc: 0.812705\n",
      "[130]\tvalid_0's auc: 0.813468\n",
      "[140]\tvalid_0's auc: 0.814403\n",
      "[150]\tvalid_0's auc: 0.81512\n",
      "[160]\tvalid_0's auc: 0.81561\n",
      "[170]\tvalid_0's auc: 0.815985\n",
      "[180]\tvalid_0's auc: 0.816284\n",
      "[190]\tvalid_0's auc: 0.816581\n",
      "[200]\tvalid_0's auc: 0.816989\n",
      "[210]\tvalid_0's auc: 0.817215\n",
      "[220]\tvalid_0's auc: 0.817553\n",
      "[230]\tvalid_0's auc: 0.817954\n",
      "[240]\tvalid_0's auc: 0.818331\n",
      "[250]\tvalid_0's auc: 0.818552\n",
      "[260]\tvalid_0's auc: 0.818878\n",
      "[270]\tvalid_0's auc: 0.819187\n",
      "[280]\tvalid_0's auc: 0.819406\n",
      "[290]\tvalid_0's auc: 0.819747\n",
      "[300]\tvalid_0's auc: 0.819937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "NUM_FOLD = 5\n",
    "kf = KFold(n_splits=NUM_FOLD,shuffle=True)\n",
    "\n",
    "gbdt_models = []\n",
    "\n",
    "for train_indices,val_indices in kf.split(train) : \n",
    "    lgb_train = lgb.Dataset(train.drop(['target'],axis=1).loc[train_indices,:],label=train.loc[train_indices,'target'])\n",
    "    lgb_val = lgb.Dataset(train.drop(['target'],axis=1).loc[val_indices,:],label=train.loc[val_indices,'target'])\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'learning_rate': 0.2 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 128,\n",
    "        'bagging_fraction': 0.95,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': 256,\n",
    "        'num_rounds': 300,\n",
    "        'metric' : 'auc',\n",
    "        } \n",
    "    \n",
    "    bst = lgb.train(params, train_set = lgb_train, valid_sets = lgb_val, verbose_eval=10)\n",
    "    gbdt_models.append(bst)\n",
    "    del bst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's auc: 0.755678\n",
      "[20]\tvalid_0's auc: 0.77465\n",
      "[30]\tvalid_0's auc: 0.78661\n",
      "[40]\tvalid_0's auc: 0.792155\n",
      "[50]\tvalid_0's auc: 0.794727\n",
      "[60]\tvalid_0's auc: 0.797523\n",
      "[70]\tvalid_0's auc: 0.798301\n",
      "[80]\tvalid_0's auc: 0.799728\n",
      "[90]\tvalid_0's auc: 0.800052\n",
      "[100]\tvalid_0's auc: 0.800947\n",
      "[110]\tvalid_0's auc: 0.801179\n",
      "[120]\tvalid_0's auc: 0.802647\n",
      "[130]\tvalid_0's auc: 0.803516\n",
      "[140]\tvalid_0's auc: 0.805016\n",
      "[150]\tvalid_0's auc: 0.805413\n",
      "[160]\tvalid_0's auc: 0.805168\n",
      "[170]\tvalid_0's auc: 0.806797\n",
      "[180]\tvalid_0's auc: 0.80696\n",
      "[190]\tvalid_0's auc: 0.808587\n",
      "[200]\tvalid_0's auc: 0.809076\n",
      "[210]\tvalid_0's auc: 0.808946\n",
      "[220]\tvalid_0's auc: 0.810357\n",
      "[230]\tvalid_0's auc: 0.81067\n",
      "[240]\tvalid_0's auc: 0.811394\n",
      "[250]\tvalid_0's auc: 0.811753\n",
      "[260]\tvalid_0's auc: 0.812268\n",
      "[270]\tvalid_0's auc: 0.812877\n",
      "[280]\tvalid_0's auc: 0.812834\n",
      "[290]\tvalid_0's auc: 0.813407\n",
      "[300]\tvalid_0's auc: 0.813387\n",
      "[10]\tvalid_0's auc: 0.756299\n",
      "[20]\tvalid_0's auc: 0.77444\n",
      "[30]\tvalid_0's auc: 0.786456\n",
      "[40]\tvalid_0's auc: 0.792931\n",
      "[50]\tvalid_0's auc: 0.795414\n",
      "[60]\tvalid_0's auc: 0.798101\n",
      "[70]\tvalid_0's auc: 0.798727\n",
      "[80]\tvalid_0's auc: 0.800197\n",
      "[90]\tvalid_0's auc: 0.800369\n",
      "[100]\tvalid_0's auc: 0.80156\n",
      "[110]\tvalid_0's auc: 0.801581\n",
      "[120]\tvalid_0's auc: 0.802821\n",
      "[130]\tvalid_0's auc: 0.803718\n",
      "[140]\tvalid_0's auc: 0.805028\n",
      "[150]\tvalid_0's auc: 0.805568\n",
      "[160]\tvalid_0's auc: 0.805222\n",
      "[170]\tvalid_0's auc: 0.806539\n",
      "[180]\tvalid_0's auc: 0.806787\n",
      "[190]\tvalid_0's auc: 0.808145\n",
      "[200]\tvalid_0's auc: 0.808842\n",
      "[210]\tvalid_0's auc: 0.808979\n",
      "[220]\tvalid_0's auc: 0.809598\n",
      "[230]\tvalid_0's auc: 0.809941\n",
      "[240]\tvalid_0's auc: 0.810827\n",
      "[250]\tvalid_0's auc: 0.811157\n",
      "[260]\tvalid_0's auc: 0.811831\n",
      "[270]\tvalid_0's auc: 0.81255\n",
      "[280]\tvalid_0's auc: 0.812612\n",
      "[290]\tvalid_0's auc: 0.813261\n",
      "[300]\tvalid_0's auc: 0.813155\n",
      "[10]\tvalid_0's auc: 0.755581\n",
      "[20]\tvalid_0's auc: 0.77425\n",
      "[30]\tvalid_0's auc: 0.786356\n",
      "[40]\tvalid_0's auc: 0.792732\n",
      "[50]\tvalid_0's auc: 0.795053\n",
      "[60]\tvalid_0's auc: 0.797533\n",
      "[70]\tvalid_0's auc: 0.798568\n",
      "[80]\tvalid_0's auc: 0.800003\n",
      "[90]\tvalid_0's auc: 0.800168\n",
      "[100]\tvalid_0's auc: 0.80139\n",
      "[110]\tvalid_0's auc: 0.801417\n",
      "[120]\tvalid_0's auc: 0.802609\n",
      "[130]\tvalid_0's auc: 0.803939\n",
      "[140]\tvalid_0's auc: 0.805168\n",
      "[150]\tvalid_0's auc: 0.8056\n",
      "[160]\tvalid_0's auc: 0.805303\n",
      "[170]\tvalid_0's auc: 0.806769\n",
      "[180]\tvalid_0's auc: 0.80692\n",
      "[190]\tvalid_0's auc: 0.808457\n",
      "[200]\tvalid_0's auc: 0.808864\n",
      "[210]\tvalid_0's auc: 0.808831\n",
      "[220]\tvalid_0's auc: 0.809794\n",
      "[230]\tvalid_0's auc: 0.810074\n",
      "[240]\tvalid_0's auc: 0.810837\n",
      "[250]\tvalid_0's auc: 0.811156\n",
      "[260]\tvalid_0's auc: 0.812031\n",
      "[270]\tvalid_0's auc: 0.812546\n",
      "[280]\tvalid_0's auc: 0.812632\n",
      "[290]\tvalid_0's auc: 0.813476\n",
      "[300]\tvalid_0's auc: 0.813366\n",
      "[10]\tvalid_0's auc: 0.75551\n",
      "[20]\tvalid_0's auc: 0.774353\n",
      "[30]\tvalid_0's auc: 0.785655\n",
      "[40]\tvalid_0's auc: 0.791853\n",
      "[50]\tvalid_0's auc: 0.794262\n",
      "[60]\tvalid_0's auc: 0.796794\n",
      "[70]\tvalid_0's auc: 0.797797\n",
      "[80]\tvalid_0's auc: 0.799214\n",
      "[90]\tvalid_0's auc: 0.799489\n",
      "[100]\tvalid_0's auc: 0.800448\n",
      "[110]\tvalid_0's auc: 0.800329\n",
      "[120]\tvalid_0's auc: 0.801737\n",
      "[130]\tvalid_0's auc: 0.802792\n",
      "[140]\tvalid_0's auc: 0.804173\n",
      "[150]\tvalid_0's auc: 0.804613\n",
      "[160]\tvalid_0's auc: 0.804366\n",
      "[170]\tvalid_0's auc: 0.805836\n",
      "[180]\tvalid_0's auc: 0.806032\n",
      "[190]\tvalid_0's auc: 0.807451\n",
      "[200]\tvalid_0's auc: 0.808177\n",
      "[210]\tvalid_0's auc: 0.808328\n",
      "[220]\tvalid_0's auc: 0.809027\n",
      "[230]\tvalid_0's auc: 0.809422\n",
      "[240]\tvalid_0's auc: 0.810193\n",
      "[250]\tvalid_0's auc: 0.810392\n",
      "[260]\tvalid_0's auc: 0.811121\n",
      "[270]\tvalid_0's auc: 0.811963\n",
      "[280]\tvalid_0's auc: 0.812034\n",
      "[290]\tvalid_0's auc: 0.812619\n",
      "[300]\tvalid_0's auc: 0.812622\n",
      "[10]\tvalid_0's auc: 0.756361\n",
      "[20]\tvalid_0's auc: 0.774772\n",
      "[30]\tvalid_0's auc: 0.786597\n",
      "[40]\tvalid_0's auc: 0.792705\n",
      "[50]\tvalid_0's auc: 0.79476\n",
      "[60]\tvalid_0's auc: 0.797663\n",
      "[70]\tvalid_0's auc: 0.79823\n",
      "[80]\tvalid_0's auc: 0.799757\n",
      "[90]\tvalid_0's auc: 0.800097\n",
      "[100]\tvalid_0's auc: 0.801072\n",
      "[110]\tvalid_0's auc: 0.801246\n",
      "[120]\tvalid_0's auc: 0.802688\n",
      "[130]\tvalid_0's auc: 0.803937\n",
      "[140]\tvalid_0's auc: 0.804974\n",
      "[150]\tvalid_0's auc: 0.805555\n",
      "[160]\tvalid_0's auc: 0.80524\n",
      "[170]\tvalid_0's auc: 0.806643\n",
      "[180]\tvalid_0's auc: 0.806773\n",
      "[190]\tvalid_0's auc: 0.808065\n",
      "[200]\tvalid_0's auc: 0.808563\n",
      "[210]\tvalid_0's auc: 0.808685\n",
      "[220]\tvalid_0's auc: 0.809617\n",
      "[230]\tvalid_0's auc: 0.809897\n",
      "[240]\tvalid_0's auc: 0.81073\n",
      "[250]\tvalid_0's auc: 0.811155\n",
      "[260]\tvalid_0's auc: 0.81183\n",
      "[270]\tvalid_0's auc: 0.812426\n",
      "[280]\tvalid_0's auc: 0.812613\n",
      "[290]\tvalid_0's auc: 0.813248\n",
      "[300]\tvalid_0's auc: 0.813182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "NUM_FOLD = 5\n",
    "kf = KFold(n_splits=NUM_FOLD,shuffle=True)\n",
    "\n",
    "dart_models = []\n",
    "\n",
    "for train_indices,val_indices in kf.split(train): \n",
    "    lgb_train = lgb.Dataset(train.drop(['target'],axis=1).loc[train_indices,:],label=train.loc[train_indices,'target'])\n",
    "    lgb_val = lgb.Dataset(train.drop(['target'],axis=1).loc[val_indices,:],label=train.loc[val_indices,'target'])\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'dart',\n",
    "        'learning_rate': 0.2 ,\n",
    "        'verbose': 5,\n",
    "        'num_leaves': 128,\n",
    "        'bagging_fraction': 0.95,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': 256,\n",
    "        'num_rounds': 300,\n",
    "        'metric' : 'auc'\n",
    "    }\n",
    "    bst = lgb.train(params, train_set = lgb_train, valid_sets = lgb_val, verbose_eval=10)\n",
    "    dart_models.append(bst)\n",
    "    del bst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_models = gbdt_models.extend(dart_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_models = gbdt_models[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gbdt_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions\n",
      "Done making predictions\n"
     ]
    }
   ],
   "source": [
    "print('Making predictions')\n",
    "\n",
    "predictions = np.zeros(shape=[len(test)])\n",
    "for model in gbdt_models:\n",
    "    predictions += model.predict(test.drop(['id'],axis=1))\n",
    "predictions/=len(gbdt_models)\n",
    "\n",
    "print('Done making predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions Model\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print ('Saving predictions Model')\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] =test['id'].values\n",
    "subm['target'] = predictions\n",
    "subm.to_csv(data_path + 'submission_lgbm_avg.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:wsdm-music-recommendation-py27]",
   "language": "python",
   "name": "conda-env-wsdm-music-recommendation-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "156px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
